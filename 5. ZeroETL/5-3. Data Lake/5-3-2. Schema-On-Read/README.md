# Schema-On-Read

* Schema-On-Read Engine은 데이터 저장 및 처리에 있어 **데이터의 스키마를 데이터 저장 시점이 아닌 읽기 시점에 적용**하는 접근 방식임.  
* 이 방법은 데이터가 저장될 때 구조나 형식을 지정하지 않고, **데이터를 사용할 때 필요한 구조로 변환하여 처리**함.  
* 이러한 접근 방식은 특히 **빅데이터 환경에서 유용**하며, **데이터 레이크**와 같은 비정형 데이터 저장소에서 널리 사용됨.

## 개념

### 정의  
데이터가 저장될 때 스키마를 미리 정의하지 않고, 데이터를 읽을 때 필요한 스키마를 적용하여 데이터에 접근하고 처리하는 방법임.

### 대조점  
Schema-On-Write와 반대되는 개념임. Schema-On-Write는 데이터를 저장할 때 미리 정의된 스키마를 따라 데이터를 저장함.

## Schema-On-Read의 작동 방식

### 데이터 저장

* 데이터는 원시 형태로 저장됨.
* 데이터 형식이나 구조에 구애받지 않고 저장소에 저장함.
* **JSON, XML, Parquet, Avro** 등의 다양한 형식의 데이터를 그대로 저장할 수 있음.

### 데이터 읽기

* 데이터를 읽을 때 **필요한 스키마를 정의**함.
* **읽기 시점에 스키마를 적용하여 데이터를 변환하고 처리**함.
* 유연하게 데이터를 다양한 구조로 변환하여 분석 및 처리할 수 있음.

## 장점

* **유연성**  
다양한 데이터 형식을 저장하고 필요에 따라 읽기 시점에 스키마를 적용하여 처리할 수 있어 유연함.

* **확장성**  
대규모 데이터 저장소에 적합하며, 스키마 변경이 빈번한 환경에서 유리함.

* **저장 속도**  
데이터를 저장할 때 스키마를 정의할 필요가 없으므로 **빠르게 저장**할 수 있음.

* **비정형 데이터 처리**  
비정형 데이터를 저장하고 처리하는데 유리함.

## 단점

* **읽기 성능**  
데이터를 읽을 때 스키마를 적용하여 변환하므로 읽기 성능이 떨어질 수 있음.

* **복잡성**  
데이터를 사용할 때마다 스키마를 정의하고 적용해야 하므로 관리가 복잡할 수 있음.

* **데이터 무결성**  
저장 시 스키마 검증을 하지 않으므로 데이터 무결성 문제가 발생할 수 있음.

## Use case

* **데이터 레이크**  
다양한 형식의 원시 데이터를 저장하고 필요에 따라 스키마를 적용하여 분석 및 처리함.

* **로그 분석**  
로그 데이터를 원시 형태로 저장하고 분석할 때 필요한 구조로 변환하여 처리함.

* **데이터 통합**  
여러 소스에서 데이터를 수집하여 저장하고, 필요한 시점에 스키마를 적용하여 통합 및 분석함.

## Schema-On-Read Engine
Apache Hadoop, Apache Hive, Apache Drill, Amazon Athena 등은 Schema-On-Read 접근 방식을 지원하는 엔진임.

## Schema 지정 방식

### 데이터 카탈로그 및 메타데이터 관리

**데이터 카탈로그**를 사용하여 데이터의 메타데이터를 관리하고, 각 데이터셋의 **스키마와 데이터 형식 정보를 유지**함.  
이를 통해 사용자는 데이터를 쿼리하기 전에 해당 데이터의 구조와 형식을 파악할 수 있음.

* **AWS Glue Data Catalog**  
    AWS Glue는 **S3에 저장된 데이터를 인덱싱**하고, **메타데이터를 자동으로 수집하여 카탈로그를 생성**함.
* **Apache Hive Metastore**  
    **Hive Metastore**는 데이터 레이크에 저장된 **데이터의 스키마와 메타데이터를 관리**하며, 다른 도구들과 통합하여 데이터를 쿼리할 수 있게 함.

### 스키마 진화 및 호환성

**스키마 진화**를 지원하여 데이터 형식이 변경되거나 새로운 필드가 추가될 때, 기존 스키마와의 호환성을 유지함.  
이를 통해 다양한 형식의 데이터를 유연하게 처리할 수 있음.

* **Avro 및 Parquet**  
    Avro와 Parquet 파일 형식은 스키마 진화를 지원하며, **데이터가 추가되거나 변경될 때도 호환성을 유지**할 수 있음.
* **Schema Registry**  
    Confluent Schema Registry와 같은 도구는 **Kafka 스트림의 스키마를 관리하고, 스키마 호환성을 보장**함.

### 데이터 변환 및 정규화

데이터를 쿼리하기 전에 **ETL**(Extract, Transform, Load) 또는 **ELT**(Extract, Load, Transform) 프로세스를 통해 데이터를 변환하고 정규화함.  
이를 통해 다양한 형식의 데이터를 일관된 구조로 변환하여 쿼리할 수 있음.

* **Apache Spark**  
    Spark는 다양한 형식의 데이터를 로드하고 변환할 수 있는 강력한 데이터 처리 엔진임. 이를 통해 **데이터를 정규화하고 공통된 스키마로 변환**할 수 있음.
* **AWS Glue**  
    Glue는 다양한 데이터 소스에서 데이터를 추출하고 변환하여 정규화된 데이터셋을 생성할 수 있음.

### 데이터 가상화

데이터 가상화 기술을 사용하여 **데이터가 실제로 저장된 위치나 형식에 상관없이, 통합된 뷰**를 제공함.  
이를 통해 다양한 형식의 데이터를 하나의 논리적 데이터 레이어로 접근할 수 있음.

* **Denodo**  
    Denodo는 다양한 데이터 소스에서 데이터를 가상화하여 통합된 데이터 뷰를 제공함.
* **Dremio**  
    Dremio는 데이터 레이크에서 다양한 형식의 데이터를 가상화하고, 빠르게 쿼리할 수 있게 해줌.

### 데이터 준비 도구

**데이터 준비(Data Preparation) 도구**를 사용하여 데이터를 시각적으로 탐색하고, **변환 및 정규화** 작업을 수행함.  
이를 통해 데이터 분석가와 엔지니어는 다양한 형식의 데이터를 일관된 스키마로 쉽게 변환할 수 있음.

* **Trifacta**  
Trifacta는 사용자가 데이터를 시각적으로 탐색하고, 변환 작업을 수행할 수 있는 데이터 준비 도구임.
* **Talend**  
Talend는 다양한 데이터 소스에서 데이터를 추출하고 변환할 수 있는 ETL 도구로, 데이터 준비 및 정규화를 지원함.

### Summary

다양한 형식의 데이터를 Data Lake에 저장하고 동일한 스키마를 적용할 수 없는 문제는 여러 방법으로 해결할 수 있음.  
**데이터 카탈로그와 메타데이터 관리**를 통해 데이터를 체계적으로 관리하고, **스키마 진화와 호환성**을 지원하는 파일 형식을 사용하며, **ETL/ELT 프로세스**를 통해 데이터를 정규화하고 변환함.  
또한 **데이터 가상화와 데이터 준비 도구**를 사용하여 데이터 접근성을 높이고, 다양한 형식의 데이터를 일관된 구조로 처리할 수 있음.  
이를 통해 **Data Lake의 유연성과 확장성을 유지하면서도 데이터를 효과적으로 관리하고 분석**할 수 있음.